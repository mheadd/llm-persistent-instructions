# LLM Provider Configuration

# Default provider to use when LLM_PROVIDER environment variable is not set
default_provider: "ollama"

# Provider configurations
providers:
  # Local Ollama configuration
  ollama:
    type: "ollama"
    url: "http://ollama:11434"  # Docker service name, fallback to localhost:11434
    model: "phi3:mini"
    timeout: 120000  # 2 minutes for model loading
    description: "Local Ollama instance with Phi-3 Mini model"
    
  # OpenAI configuration  
  openai:
    type: "openai"
    model: "gpt-3.5-turbo"
    apiKeyEnv: "OPENAI_API_KEY"  # Environment variable name for API key
    timeout: 30000  # 30 seconds
    maxRetries: 3
    description: "OpenAI GPT-3.5 Turbo model"
    
  # OpenAI GPT-4 configuration (alternative)
  openai-gpt4:
    type: "openai"
    model: "gpt-4"
    apiKeyEnv: "OPENAI_API_KEY"
    timeout: 60000  # 1 minute for GPT-4
    maxRetries: 3
    description: "OpenAI GPT-4 model (more capable, slower)"

# Future provider configurations can be added here:
#
# anthropic:
#   type: "anthropic"
#   model: "claude-3-haiku-20240307"
#   apiKeyEnv: "ANTHROPIC_API_KEY"
#   timeout: 30000
#   description: "Anthropic Claude 3 Haiku model"
#   
# azure:
#   type: "azure"
#   endpoint: "https://your-resource.openai.azure.com/"
#   apiKeyEnv: "AZURE_OPENAI_API_KEY"
#   deployment: "gpt-35-turbo"
#   apiVersion: "2023-12-01-preview"
#   timeout: 30000
#   description: "Azure OpenAI GPT-3.5 Turbo"

# Fallback configuration
# If the primary provider fails, these will be tried in order
fallback_providers:
  - "ollama"
  - "openai"

# Global settings
settings:
  # Default generation parameters (can be overridden per request)
  default_temperature: 0.7
  default_max_tokens: 300
  default_top_p: 0.9
  
  # Request timeouts
  health_check_timeout: 5000
  generation_timeout: 120000
  
  # Retry settings
  max_retries: 3
  retry_delay: 1000  # milliseconds
  
  # Logging
  log_provider_usage: true
  log_response_times: true
