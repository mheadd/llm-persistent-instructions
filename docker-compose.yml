services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-service
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=60m          # Keep model in memory for 60 minutes
      - OLLAMA_NUM_PARALLEL=2          # Process multiple requests in parallel
      - OLLAMA_MAX_LOADED_MODELS=1     # Keep only 1 model loaded to save memory
    deploy:
      resources:
        limits:
          memory: 8G                   # Maximum memory allocation
        reservations:
          memory: 6G                   # Reserved memory allocation
    shm_size: 2G                       # Shared memory for faster processing
    networks:
      - ai-network
    restart: unless-stopped

  api:
    build: ./api
    container_name: government-ai-api
    ports:
      - "3000:3000"
    volumes:
      - ./api/config:/app/config
    environment:
      - NODE_ENV=production
      - OLLAMA_URL=http://ollama:11434
    depends_on:
      - ollama
    deploy:
      resources:
        limits:
          memory: 1G                   # API doesn't need much memory
        reservations:
          memory: 512M
    networks:
      - ai-network
    restart: unless-stopped

volumes:
  ollama_data:
    driver: local

networks:
  ai-network:
    driver: bridge